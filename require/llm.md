# LinQ LLM ì‹œìŠ¤í…œ ëª…ì„¸ì„œ

## 0. í˜„ì¬ ê°œë°œ ìƒí™© (2024ë…„ 12ì›” ì—…ë°ì´íŠ¸)

**í˜„ì¬ ìƒíƒœ: 0% (ì„¤ê³„ ë‹¨ê³„)** **ìš°ì„ ìˆœìœ„: ì¤‘ê¸° ê³„íš (ë°±ì—”ë“œ ì—°ë™ í›„ êµ¬í˜„ ì˜ˆì •)**

### ğŸ“Š í”„ë¡ íŠ¸ì—”ë“œ ì™„ì„±ë„ ë° AI ê¸°ëŠ¥ ì¤€ë¹„ ìƒí™©

#### âœ… í”„ë¡ íŠ¸ì—”ë“œì—ì„œ AI ê¸°ëŠ¥ ì¤€ë¹„ëœ ë¶€ë¶„

- **ìì—°ì–´ ì…ë ¥ UI**: AddEventModalì—ì„œ ì œëª© ì…ë ¥ í•„ë“œ êµ¬í˜„ë¨
- **ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜**: 'work', 'health', 'social', 'personal' ì‹œìŠ¤í…œ êµ¬ì¶•
- **ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ**: 'HIGH', 'MEDIUM', 'LOW' ì¸í„°í˜ì´ìŠ¤ ì¤€ë¹„ë¨
- **AI ì±„íŒ… í™”ë©´**: `app/(tabs)/chat.tsx` íŒŒì¼ êµ¬ì¡° ì¤€ë¹„ë¨

#### ğŸ”„ í˜„ì¬ ê°œë°œ ìš°ì„ ìˆœìœ„

1. **Phase 1 (í˜„ì¬)**: ë°±ì—”ë“œ ê¸°ë³¸ ì¸í”„ë¼ êµ¬ì¶•
2. **Phase 2 (1-2ì£¼ í›„)**: ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ
3. **Phase 3 (2-4ì£¼ í›„)**: **AI ê¸°ëŠ¥ êµ¬í˜„ ì‹œì‘** â† ì´ ë‹¨ê³„
4. **Phase 4 (4-8ì£¼ í›„)**: ê³ ê¸‰ AI ê¸°ëŠ¥ ë° ìµœì í™”

#### ğŸ¯ AI ê¸°ëŠ¥ êµ¬í˜„ ëª©í‘œ

í”„ë¡ íŠ¸ì—”ë“œê°€ **MVP ìƒíƒœë¡œ ì™„ì„±**ë˜ì–´ ìˆì–´, AI ê¸°ëŠ¥ êµ¬í˜„ ì‹œ ì¦‰ì‹œ ì‚¬ìš©ìì—ê²Œ ì œê³µ
ê°€ëŠ¥í•œ ìƒíƒœì…ë‹ˆë‹¤. ë°±ì—”ë“œ ì—°ë™ ì™„ë£Œ í›„ ë‹¤ìŒ AI ê¸°ëŠ¥ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬í˜„ ì˜ˆì •:

1. **ìì—°ì–´ ì¼ì • íŒŒì‹±**: "ë‚´ì¼ ì˜¤í›„ 2ì‹œì— íšŒì˜"
2. **AI ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜**: ì œëª© ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ìë™ ì„¤ì •
3. **AI ìš°ì„ ìˆœìœ„ ë¶„ì„**: ë§¥ë½ ê¸°ë°˜ ì¤‘ìš”ë„ ìë™ íŒë‹¨
4. **AI ì±—ë´‡**: `app/(tabs)/chat.tsx` í™”ë©´ í™œì„±í™”

---

## 1. LLM ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 ëª¨ë¸ ì„ íƒ: Solar Pro

**Upstage Solar Pro**ë¥¼ ì£¼ë ¥ ëª¨ë¸ë¡œ ì„ íƒí•œ ì´ìœ :

- **í•œêµ­ì–´ íŠ¹í™”**: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ì„±ëŠ¥ ìš°ìˆ˜
- **ì»¨í…ìŠ¤íŠ¸ ì´í•´**: ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ëŠ¥ë ¥ (ìµœëŒ€ 128K í† í°)
- **í•¨ìˆ˜ í˜¸ì¶œ**: Function Calling ì§€ì›ìœ¼ë¡œ ì¼ì • ìƒì„±/ìˆ˜ì • ê°€ëŠ¥
- **ë¹„ìš© íš¨ìœ¨**: OpenAI ëŒ€ë¹„ í•©ë¦¬ì ì¸ ê°€ê²© ì •ì±…
- **ì§€ì—°ì‹œê°„**: í•œêµ­ ì„œë²„ ìœ„ì¹˜ë¡œ ë‚®ì€ ë ˆì´í„´ì‹œ

### 1.2 í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì „ëµ

```
Primary: Solar Pro (í•œêµ­ì–´ NLU/NLG)
    â†“
Fallback: GPT-4 (ë³µì¡í•œ ì¶”ë¡ )
    â†“
Specialized: Embedding Model (ë²¡í„° ê²€ìƒ‰)
```

### 1.3 LangChain ì•„í‚¤í…ì²˜

```python
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Layer   â”‚    â”‚  Processing     â”‚    â”‚  Output Layer   â”‚
â”‚                 â”‚    â”‚     Chain       â”‚    â”‚                 â”‚
â”‚ - Text Input    â”‚â”€â”€â”€â–¶â”‚ - Intent        â”‚â”€â”€â”€â–¶â”‚ - Structured    â”‚
â”‚ - Voice Input   â”‚    â”‚ - Entity Extractâ”‚    â”‚   Response      â”‚
â”‚ - Context       â”‚    â”‚ - Function Call â”‚    â”‚ - Event Object  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   Memory &      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚   Vector Store   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ìì—°ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 2.1 ì…ë ¥ ì²˜ë¦¬ ì²´ì¸

```python
# langchain_pipeline.py
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.agents import AgentExecutor, Tool
from langchain.schema import BaseOutputParser

class ScheduleInputChain:
    def __init__(self):
        self.solar_llm = SolarProLLM(
            api_key=os.getenv("UPSTAGE_API_KEY"),
            model="solar-1-mini-chat",
            temperature=0.1,
            max_tokens=1024
        )

        self.memory = ConversationBufferWindowMemory(
            k=5,  # ìµœê·¼ 5ê°œ ëŒ€í™” ê¸°ì–µ
            memory_key="chat_history",
            return_messages=True
        )

        self.chain = self._create_processing_chain()

    def _create_processing_chain(self):
        prompt = PromptTemplate(
            input_variables=["user_input", "current_context", "chat_history"],
            template="""
ë‹¹ì‹ ì€ í•œêµ­ì–´ ì¼ì • ê´€ë¦¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìì—°ì–´ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì¼ì • ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

í˜„ì¬ ì»¨í…ìŠ¤íŠ¸:
- í˜„ì¬ ì‹œê°„: {current_context[current_time]}
- ì‚¬ìš©ì ìœ„ì¹˜: {current_context[user_location]}
- ìµœê·¼ ì¼ì •: {current_context[recent_events]}

ëŒ€í™” ê¸°ë¡:
{chat_history}

ì‚¬ìš©ì ì…ë ¥: {user_input}

ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”:
1. intent: ì˜ë„ (create_event, modify_event, query_schedule, general_chat)
2. entities: ì¶”ì¶œëœ ì—”í‹°í‹°ë“¤
3. event_details: ì¼ì • ìƒì„¸ ì •ë³´ (í•´ë‹¹ì‹œ)
4. confidence: ì‹ ë¢°ë„ (0-1)
5. response: ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì‘ë‹µ

ì‘ë‹µ:
"""
        )

        return LLMChain(
            llm=self.solar_llm,
            prompt=prompt,
            memory=self.memory,
            verbose=True
        )
```

### 2.2 ì—”í‹°í‹° ì¶”ì¶œ ì‹œìŠ¤í…œ

```python
class EntityExtractor:
    def __init__(self):
        self.extractors = {
            'datetime': DateTimeExtractor(),
            'location': LocationExtractor(),
            'person': PersonExtractor(),
            'duration': DurationExtractor(),
            'category': CategoryExtractor(),
            'priority': PriorityAnalyzer()
        }

    async def extract_entities(self, text: str, context: dict) -> dict:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì¼ì • ê´€ë ¨ ì—”í‹°í‹° ì¶”ì¶œ"""

        # Solar Proë¥¼ í™œìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ
        extraction_prompt = f"""
ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì¼ì • ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

í…ìŠ¤íŠ¸: "{text}"
í˜„ì¬ ì‹œê°„: {context.get('current_time')}
ì‚¬ìš©ì ìœ„ì¹˜: {context.get('user_location')}

ì¶”ì¶œí•  ì •ë³´:
1. ì‹œê°„ ì •ë³´ (ë‚ ì§œ, ì‹œê°, ê¸°ê°„)
2. ì¥ì†Œ ì •ë³´ (êµ¬ì²´ì  ì£¼ì†Œ, ì¥ì†Œëª…, ì˜¨ë¼ì¸ ì—¬ë¶€)
3. ì°¸ì„ì ì •ë³´ (ì´ë¦„, ì—­í• )
4. ì¼ì • ì œëª©/ë‚´ìš©
5. ìš°ì„ ìˆœìœ„/ì¤‘ìš”ë„ (ìƒ/ì¤‘/í•˜ ìë™ ë¶„ì„)
6. ì¹´í…Œê³ ë¦¬ (ì—…ë¬´, ê°œì¸, ê±´ê°•, ì‚¬êµ, ì—¬í–‰)

JSON í˜•íƒœë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "datetime": {{
        "start_time": "ISO 8601 í˜•ì‹",
        "end_time": "ISO 8601 í˜•ì‹",
        "is_all_day": boolean,
        "timezone": "Asia/Seoul"
    }},
    "location": {{
        "type": "physical|virtual|hybrid",
        "address": "ì£¼ì†Œ",
        "name": "ì¥ì†Œëª…",
        "coordinates": {{"lat": 0, "lng": 0}}
    }},
    "title": "ì¼ì • ì œëª©",
    "description": "ìƒì„¸ ì„¤ëª…",
    "category": "WORK|PERSONAL|HEALTH|SOCIAL|TRAVEL",
    "priority": "LOW|MEDIUM|HIGH",
    "ai_priority_score": 0.75,
    "priority_reasoning": "ì—…ë¬´ ê´€ë ¨ íšŒì˜ë¡œ ì°¸ì„ìê°€ ë§ì•„ ì¤‘ìš”ë„ ë†’ìŒ",
    "participants": [
        {{"name": "ì°¸ì„ìëª…", "role": "organizer|required|optional"}}
    ],
    "confidence": 0.95
}}
"""

        response = await self.solar_llm.agenerate([extraction_prompt])
        return self._parse_extraction_response(response.generations[0][0].text)

    async def _analyze_priority(self, event_data: dict, context: dict) -> dict:
        """AI ê¸°ë°˜ ì¤‘ìš”ë„ ìë™ ë¶„ì„"""

        priority_prompt = f"""
ë‹¤ìŒ ì¼ì •ì˜ ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•˜ì—¬ ìƒ/ì¤‘/í•˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

ì¼ì • ì •ë³´:
- ì œëª©: {event_data.get('title', '')}
- ë‚´ìš©: {event_data.get('description', '')}
- ì¹´í…Œê³ ë¦¬: {event_data.get('category', '')}
- ì°¸ì„ì: {event_data.get('participants', [])}
- ì‹œê°„: {event_data.get('datetime', {})}

ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸:
- í˜„ì¬ ì‹œê°„: {context.get('current_time')}
- ìµœê·¼ ì¼ì •: {context.get('recent_events', [])}
- ì‚¬ìš©ì ì„ í˜¸ë„: {context.get('user_preferences', {})}

ì¤‘ìš”ë„ íŒë‹¨ ê¸°ì¤€:
1. ìƒ(HIGH):
   - ì¤‘ìš”í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¯¸íŒ…, ë°œí‘œ, ë©´ì ‘
   - ì˜ë£Œ ì˜ˆì•½, ë²•ì  ì˜ë¬´ì‚¬í•­
   - ë°ë“œë¼ì¸ì´ ìˆëŠ” ì¤‘ìš” ì—…ë¬´
   - ë‹¤ìˆ˜ ì°¸ì„ìê°€ ìˆëŠ” íšŒì˜

2. ì¤‘(MEDIUM):
   - ì¼ë°˜ ì—…ë¬´ ë¯¸íŒ…, ì •ê¸° íšŒì˜
   - ê°œì¸ ì•½ì†, ì‚¬êµ ëª¨ì„
   - í•™ìŠµ/êµìœ¡ ê´€ë ¨ ì¼ì •
   - ë£¨í‹´í•œ ì—…ë¬´

3. í•˜(LOW):
   - ê°œì¸ ì—¬ê°€ í™œë™
   - ì„ íƒì  ì°¸ì„ ì´ë²¤íŠ¸
   - ìœ ì—°í•˜ê²Œ ì¡°ì • ê°€ëŠ¥í•œ ì¼ì •
   - ì·¨ë¯¸/ì˜¤ë½ í™œë™

ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì‘ë‹µ:
{{
    "priority": "HIGH|MEDIUM|LOW",
    "confidence_score": 0.85,
    "reasoning": "íŒë‹¨ ê·¼ê±° ì„¤ëª…",
    "key_factors": ["ì¤‘ìš”ë„ì— ì˜í–¥ì„ ì¤€ ì£¼ìš” ìš”ì†Œë“¤"]
}}
"""

        response = await self.solar_llm.agenerate([priority_prompt])
        return json.loads(response.generations[0][0].text)
```

### 2.3 ì˜ë„ ë¶„ë¥˜ ì‹œìŠ¤í…œ

```python
class IntentClassifier:
    def __init__(self):
        self.intent_chain = self._create_intent_chain()

    def _create_intent_chain(self):
        intent_prompt = PromptTemplate(
            input_variables=["user_input", "context"],
            template="""
ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.

ê°€ëŠ¥í•œ ì˜ë„:
1. CREATE_EVENT: ìƒˆë¡œìš´ ì¼ì • ìƒì„±
   - ì˜ˆ: "ë‚´ì¼ 2ì‹œì— íšŒì˜ ì¡ì•„ì¤˜", "ë‹¤ìŒì£¼ ì¹˜ê³¼ ì˜ˆì•½"

2. MODIFY_EVENT: ê¸°ì¡´ ì¼ì • ìˆ˜ì •
   - ì˜ˆ: "íšŒì˜ ì‹œê°„ 3ì‹œë¡œ ë°”ê¿”ì¤˜", "ë‚´ì¼ ì•½ì† ì·¨ì†Œí•´ì¤˜"

3. QUERY_SCHEDULE: ì¼ì • ì¡°íšŒ
   - ì˜ˆ: "ë‚´ì¼ ì¼ì • ë­ì•¼?", "ì´ë²ˆì£¼ íšŒì˜ ëª‡ ê°œì•¼?"

4. SUGGEST_OPTIMIZATION: ì¼ì • ìµœì í™” ìš”ì²­
   - ì˜ˆ: "ì¼ì • ì •ë¦¬í•´ì¤˜", "ì‹œê°„ ì¢€ íš¨ìœ¨ì ìœ¼ë¡œ ë°°ì¹˜í•´ì¤˜"

5. CONFLICT_RESOLUTION: ì¶©ëŒ í•´ê²°
   - ì˜ˆ: "ê²¹ì¹˜ëŠ” ì¼ì • ì–´ë–»ê²Œ í•˜ì§€?", "ì‹œê°„ ì¡°ì •í•´ì¤˜"

6. GENERAL_CHAT: ì¼ë°˜ ëŒ€í™”
   - ì˜ˆ: "ì•ˆë…•", "ê³ ë§ˆì›Œ", "ë„ì›€ë§"

ì‚¬ìš©ì ì…ë ¥: "{user_input}"
ì»¨í…ìŠ¤íŠ¸: {context}

ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì‘ë‹µ:
{{
    "intent": "ì˜ë„ ë¶„ë¥˜",
    "confidence": 0.95,
    "sub_intent": "ì„¸ë¶€ ì˜ë„ (ì„ íƒì )",
    "entities_needed": ["í•„ìš”í•œ ì¶”ê°€ ì •ë³´"],
    "reasoning": "ë¶„ë¥˜ ê·¼ê±°"
}}
"""
        )

        return LLMChain(
            llm=self.solar_llm,
            prompt=intent_prompt,
            verbose=True
        )
```

---

## 3. AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

### 3.1 ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜

```python
class LinQAgentSystem:
    def __init__(self):
        self.agents = {
            'scheduler': SchedulerAgent(),      # ì¼ì • ìƒì„±/ê´€ë¦¬
            'optimizer': OptimizerAgent(),      # ì¼ì • ìµœì í™”
            'analyzer': AnalyzerAgent(),        # íŒ¨í„´ ë¶„ì„
            'assistant': AssistantAgent()       # ì¼ë°˜ ëŒ€í™”
        }

        self.router = AgentRouter()
        self.coordinator = AgentCoordinator()

    async def process_request(self, user_input: str, context: dict):
        # 1. ì˜ë„ íŒŒì•… ë° ì—ì´ì „íŠ¸ ë¼ìš°íŒ…
        intent = await self.router.classify_intent(user_input, context)

        # 2. ì ì ˆí•œ ì—ì´ì „íŠ¸ ì„ íƒ
        primary_agent = self.agents[intent.primary_agent]

        # 3. ì—ì´ì „íŠ¸ ì‹¤í–‰
        response = await primary_agent.execute(user_input, context)

        # 4. í•„ìš”ì‹œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì™€ í˜‘ì—…
        if response.needs_collaboration:
            response = await self.coordinator.collaborate(
                primary_agent,
                response,
                context
            )

        return response
```

### 3.2 ìŠ¤ì¼€ì¤„ëŸ¬ ì—ì´ì „íŠ¸

```python
class SchedulerAgent:
    def __init__(self):
        self.tools = [
            Tool(
                name="create_event",
                description="ìƒˆë¡œìš´ ì¼ì •ì„ ìƒì„±í•©ë‹ˆë‹¤",
                func=self._create_event
            ),
            Tool(
                name="modify_event",
                description="ê¸°ì¡´ ì¼ì •ì„ ìˆ˜ì •í•©ë‹ˆë‹¤",
                func=self._modify_event
            ),
            Tool(
                name="check_conflicts",
                description="ì¼ì • ì¶©ëŒì„ í™•ì¸í•©ë‹ˆë‹¤",
                func=self._check_conflicts
            ),
            Tool(
                name="suggest_time",
                description="ìµœì ì˜ ì‹œê°„ì„ ì œì•ˆí•©ë‹ˆë‹¤",
                func=self._suggest_optimal_time
            )
        ]

        self.agent = initialize_agent(
            tools=self.tools,
            llm=self.solar_llm,
            agent=AgentType.OPENAI_FUNCTIONS,
            verbose=True,
            memory=self.memory
        )

    async def _create_event(self, event_data: str) -> str:
        """ì¼ì • ìƒì„± ë„êµ¬ í•¨ìˆ˜"""
        try:
            # 1. ì…ë ¥ ë°ì´í„° íŒŒì‹±
            parsed_data = json.loads(event_data)

            # 2. ì¶©ëŒ ê²€ì‚¬
            conflicts = await self._check_conflicts(parsed_data)

            if conflicts:
                # 3. ì¶©ëŒ í•´ê²° ì œì•ˆ
                suggestions = await self._generate_conflict_resolutions(
                    parsed_data, conflicts
                )
                return f"ì¼ì • ì¶©ëŒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì œì•ˆì‚¬í•­: {suggestions}"

            # 4. ì¼ì • ìƒì„±
            event = await self.event_service.create_event(parsed_data)

            # 5. ìŠ¤ë§ˆíŠ¸ ì•Œë¦¼ ì„¤ì •
            await self.notification_service.schedule_smart_reminders(event)

            return f"ì¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {event.title}"

        except Exception as e:
            return f"ì¼ì • ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _suggest_optimal_time(self, requirements: str) -> str:
        """ìµœì  ì‹œê°„ ì œì•ˆ"""

        suggestion_prompt = f"""
ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì¼ì • ì‹œê°„ì„ ì œì•ˆí•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­: {requirements}
ì‚¬ìš©ì íŒ¨í„´: {await self._get_user_patterns()}
ê¸°ì¡´ ì¼ì •: {await self._get_existing_schedule()}
ì„ í˜¸ë„: {await self._get_user_preferences()}

ê³ ë ¤ì‚¬í•­:
1. ì‚¬ìš©ìì˜ ìƒì‚°ì„± íŒ¨í„´ (í”¼í¬ ì‹œê°„ëŒ€)
2. ê¸°ì¡´ ì¼ì •ê³¼ì˜ ì¶©ëŒ ë°©ì§€
3. ì´ë™ ì‹œê°„ ê³ ë ¤
4. íœ´ì‹ ì‹œê°„ í™•ë³´
5. ì—ë„ˆì§€ ë ˆë²¨ ìµœì í™”

3ê°€ì§€ ì‹œê°„ëŒ€ë¥¼ ì¶”ì²œí•˜ê³  ê°ê°ì˜ ì¥ë‹¨ì ì„ ì„¤ëª…í•˜ì„¸ìš”.
"""

        response = await self.solar_llm.agenerate([suggestion_prompt])
        return response.generations[0][0].text
```

### 3.3 ìµœì í™” ì—ì´ì „íŠ¸

```python
class OptimizerAgent:
    def __init__(self):
        self.optimization_tools = [
            Tool(
                name="analyze_schedule_efficiency",
                description="ì¼ì • íš¨ìœ¨ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤",
                func=self._analyze_efficiency
            ),
            Tool(
                name="suggest_reorganization",
                description="ì¼ì • ì¬êµ¬ì„±ì„ ì œì•ˆí•©ë‹ˆë‹¤",
                func=self._suggest_reorganization
            ),
            Tool(
                name="optimize_travel_time",
                description="ì´ë™ ì‹œê°„ì„ ìµœì í™”í•©ë‹ˆë‹¤",
                func=self._optimize_travel
            )
        ]

    async def _suggest_reorganization(self, schedule_data: str) -> str:
        """ì¼ì • ì¬êµ¬ì„± ì œì•ˆ"""

        optimization_prompt = f"""
ë‹¤ìŒ ì¼ì •ì„ ë¶„ì„í•˜ì—¬ ìµœì í™” ë°©ì•ˆì„ ì œì•ˆí•˜ì„¸ìš”:

í˜„ì¬ ì¼ì •: {schedule_data}
ì‚¬ìš©ì íŒ¨í„´: {await self._get_productivity_patterns()}

ìµœì í™” ê¸°ì¤€:
1. ìƒì‚°ì„± ê·¹ëŒ€í™”
2. ì´ë™ ì‹œê°„ ìµœì†Œí™”
3. ì—ë„ˆì§€ íš¨ìœ¨ì„±
4. íœ´ì‹ ì‹œê°„ í™•ë³´
5. ìš°ì„ ìˆœìœ„ ê³ ë ¤

êµ¬ì²´ì ì¸ ì¬êµ¬ì„± ë°©ì•ˆì„ ì œì‹œí•˜ê³ , ì˜ˆìƒë˜ëŠ” ê°œì„  íš¨ê³¼ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
ë³€ê²½ì´ í•„ìš”í•œ ì¼ì •ê³¼ ê·¸ ì´ìœ ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""

                 response = await self.solar_llm.agenerate([optimization_prompt])
         return response.generations[0][0].text
```

### 3.4 ì¤‘ìš”ë„ ë¶„ì„ ì—ì´ì „íŠ¸

```python
class PriorityAnalysisAgent:
    def __init__(self):
        self.priority_model = SolarProLLM(
            model="solar-1-mini-chat",
            temperature=0.1  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
        )

        self.priority_factors = {
            'deadline_urgency': 0.25,      # ë§ˆê°ì¼ ì„ë°•ë„
            'participant_importance': 0.20, # ì°¸ì„ì ì¤‘ìš”ë„
            'business_impact': 0.20,       # ì—…ë¬´ ì˜í–¥ë„
            'category_weight': 0.15,       # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì¤‘ì¹˜
            'user_history': 0.10,          # ì‚¬ìš©ì ê³¼ê±° íŒ¨í„´
            'time_conflict': 0.10          # ì‹œê°„ ì¶©ëŒ ì •ë„
        }

    async def analyze_priority(self, event_data: dict, context: dict) -> dict:
        """ì¢…í•©ì ì¸ ì¤‘ìš”ë„ ë¶„ì„"""

        # 1. ê°œë³„ ìš”ì†Œ ë¶„ì„
        factor_scores = await self._analyze_individual_factors(event_data, context)

        # 2. ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = sum(
            factor_scores[factor] * weight
            for factor, weight in self.priority_factors.items()
        )

        # 3. ì ìˆ˜ë¥¼ ìƒ/ì¤‘/í•˜ë¡œ ë³€í™˜
        priority_level = self._score_to_priority(weighted_score)

        # 4. ì„¤ëª… ìƒì„±
        reasoning = await self._generate_reasoning(factor_scores, priority_level)

        return {
            'priority': priority_level,
            'confidence': self._calculate_confidence(factor_scores),
            'reasoning': reasoning,
            'factor_breakdown': factor_scores,
            'weighted_score': weighted_score
        }

    def _score_to_priority(self, score: float) -> str:
        """ì ìˆ˜ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ë³€í™˜"""
        if score >= 0.7:
            return 'HIGH'
        elif score >= 0.4:
            return 'MEDIUM'
        else:
            return 'LOW'

    async def _analyze_individual_factors(self, event_data: dict, context: dict) -> dict:
        """ê°œë³„ ìš”ì†Œ ë¶„ì„"""

        analysis_prompt = f"""
ë‹¤ìŒ ì¼ì •ì˜ ê° ìš”ì†Œë¥¼ 0-1 ì ìˆ˜ë¡œ ë¶„ì„í•˜ì„¸ìš”:

ì¼ì • ì •ë³´: {event_data}
ì»¨í…ìŠ¤íŠ¸: {context}

ë¶„ì„ ìš”ì†Œ:
1. deadline_urgency: ë§ˆê°ì¼ì´ë‚˜ ì‹œê°„ì˜ ê¸´ê¸‰ì„±
2. participant_importance: ì°¸ì„ìì˜ ì¤‘ìš”ë„
3. business_impact: ì—…ë¬´ë‚˜ ê°œì¸ ìƒí™œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. category_weight: ì¹´í…Œê³ ë¦¬ë³„ ì¼ë°˜ì  ì¤‘ìš”ë„
5. user_history: ì‚¬ìš©ìì˜ ê³¼ê±° ìœ ì‚¬ ì¼ì • íŒ¨í„´
6. time_conflict: ë‹¤ë¥¸ ì¼ì •ê³¼ì˜ ì¶©ëŒ ê°€ëŠ¥ì„±

JSON í˜•íƒœë¡œ ì‘ë‹µ:
{{
    "deadline_urgency": 0.8,
    "participant_importance": 0.6,
    "business_impact": 0.9,
    "category_weight": 0.7,
    "user_history": 0.5,
    "time_conflict": 0.3,
    "analysis_notes": "ê° ì ìˆ˜ì˜ ê·¼ê±°"
}}
"""

        response = await self.priority_model.agenerate([analysis_prompt])
        return json.loads(response.generations[0][0].text)

    async def learn_from_user_feedback(self, event_id: str, user_correction: str):
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ í•™ìŠµ"""

        # ì‚¬ìš©ìê°€ ì¤‘ìš”ë„ë¥¼ ìˆ˜ì •í•œ ê²½ìš° í•™ìŠµ
        feedback_data = {
            'event_id': event_id,
            'original_analysis': await self._get_original_analysis(event_id),
            'user_correction': user_correction,
            'timestamp': datetime.now()
        }

        # í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥í•˜ì—¬ í–¥í›„ ë¶„ì„ ê°œì„ 
        await self._update_learning_model(feedback_data)
```

---

## 4. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

### 4.1 ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ê³„

```python
SYSTEM_PROMPTS = {
    "scheduler": """
ë‹¹ì‹ ì€ LinQì˜ ì „ë¬¸ ì¼ì • ê´€ë¦¬ AIì…ë‹ˆë‹¤.

ì—­í• ê³¼ ì±…ì„:
- í•œêµ­ì–´ ìì—°ì–´ ì…ë ¥ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì¼ì •ìœ¼ë¡œ ë³€í™˜
- ì‚¬ìš©ìì˜ íŒ¨í„´ê³¼ ì„ í˜¸ë„ë¥¼ í•™ìŠµí•˜ì—¬ ê°œì¸í™”ëœ ì œì•ˆ
- ì¼ì • ì¶©ëŒì„ ë¯¸ë¦¬ ê°ì§€í•˜ê³  í•´ê²°ì±… ì œì‹œ
- íš¨ìœ¨ì ì¸ ì‹œê°„ ê´€ë¦¬ë¥¼ ìœ„í•œ ì¡°ì–¸ ì œê³µ

ì‘ë‹µ ì›ì¹™:
1. ì •í™•ì„±: ì‹œê°„, ì¥ì†Œ, ì°¸ì„ì ì •ë³´ë¥¼ ì •í™•íˆ íŒŒì•…
2. ê°œì¸í™”: ì‚¬ìš©ìì˜ ê³¼ê±° íŒ¨í„´ê³¼ ì„ í˜¸ë„ ë°˜ì˜
3. ëŠ¥ë™ì„±: ì ì¬ì  ë¬¸ì œë¥¼ ë¯¸ë¦¬ íŒŒì•…í•˜ê³  í•´ê²°ì±… ì œì‹œ
4. ì¹œê·¼í•¨: ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì†Œí†µ

ì œì•½ì‚¬í•­:
- ê°œì¸ì •ë³´ëŠ” ì ˆëŒ€ ì™¸ë¶€ ìœ ì¶œ ê¸ˆì§€
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì‚¬ìš©ìì—ê²Œ í™•ì¸ ìš”ì²­
- ì˜ë£Œ/ë²•ë¥  ì¡°ì–¸ì€ ì œê³µí•˜ì§€ ì•ŠìŒ
""",

    "optimizer": """
ë‹¹ì‹ ì€ ì¼ì • ìµœì í™” ì „ë¬¸ AIì…ë‹ˆë‹¤.

ìµœì í™” ëª©í‘œ:
1. ìƒì‚°ì„± ê·¹ëŒ€í™”
2. ìŠ¤íŠ¸ë ˆìŠ¤ ìµœì†Œí™”
3. ì¼ê³¼ ì‚¶ì˜ ê· í˜•
4. ì—ë„ˆì§€ íš¨ìœ¨ì„±

ë¶„ì„ ìš”ì†Œ:
- ì‹œê°„ëŒ€ë³„ ìƒì‚°ì„± íŒ¨í„´
- ì´ë™ ê²½ë¡œ ìµœì í™”
- íœ´ì‹ ì‹œê°„ ë°°ë¶„
- ì—…ë¬´ ìœ í˜•ë³„ ì§‘ì¤‘ë„
- ê°œì¸ì  ì„ í˜¸ë„

ì œì•ˆ í˜•ì‹:
- êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©ì•ˆ
- ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì˜ˆìƒ íš¨ê³¼
- ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš
""",

    "analyzer": """
ë‹¹ì‹ ì€ ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¶„ì„ ì˜ì—­:
1. ì‹œê°„ ì‚¬ìš© íŒ¨í„´
2. ìƒì‚°ì„± ì‚¬ì´í´
3. ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸
4. ëª©í‘œ ë‹¬ì„±ë„
5. ìŠµê´€ í˜•ì„±

ì¸ì‚¬ì´íŠ¸ ìƒì„±:
- ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„
- ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆ
- ì¥ê¸°ì  ë°œì „ ê³„íš
- ë™ê¸°ë¶€ì—¬ ìš”ì†Œ í¬í•¨

ë³´ê³  ì›ì¹™:
- ê¸ì •ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê·¼ê±° ì œì‹œ
- ì ì§„ì  ê°œì„  ì¤‘ì‹¬
"""
}
```

### 4.2 Few-Shot í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
FEW_SHOT_EXAMPLES = {
    "event_creation": [
        {
            "input": "ë‚´ì¼ ì˜¤í›„ 3ì‹œì— ê¹€ê³¼ì¥ë‹˜ê³¼ í”„ë¡œì íŠ¸ íšŒì˜",
            "output": {
                "intent": "CREATE_EVENT",
                "title": "í”„ë¡œì íŠ¸ íšŒì˜",
                "start_time": "2024-01-16T15:00:00+09:00",
                "participants": [{"name": "ê¹€ê³¼ì¥", "role": "required"}],
                "category": "WORK",
                "confidence": 0.95
            }
        },
        {
            "input": "ë‹¤ìŒì£¼ í™”ìš”ì¼ ì €ë…ì— ì¹œêµ¬ë“¤ê³¼ ì €ë…ì‹ì‚¬",
            "output": {
                "intent": "CREATE_EVENT",
                "title": "ì¹œêµ¬ë“¤ê³¼ ì €ë…ì‹ì‚¬",
                "start_time": "2024-01-23T18:00:00+09:00",
                "category": "SOCIAL",
                "confidence": 0.88
            }
        }
    ],

    "schedule_optimization": [
        {
            "input": "ì˜¤ëŠ˜ ì¼ì •ì´ ë„ˆë¬´ ë¹¡ë¹¡í•´ì„œ ì •ë¦¬í•´ì¤˜",
            "analysis": "3ì‹œê°„ ì—°ì† íšŒì˜, ì´ë™ì‹œê°„ ë¯¸ê³ ë ¤, ì ì‹¬ì‹œê°„ ì—†ìŒ",
            "suggestions": [
                "íšŒì˜ ì‚¬ì´ 15ë¶„ íœ´ì‹ ì¶”ê°€",
                "ì ì‹¬ì‹œê°„ 1ì‹œê°„ í™•ë³´",
                "í™”ìƒíšŒì˜ë¡œ ì´ë™ì‹œê°„ ë‹¨ì¶•"
            ]
        }
    ]
}
```

### 4.3 ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

```python
class ContextManager:
    def __init__(self):
        self.context_window = 4096  # Solar Pro ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
        self.context_cache = {}

    def build_context(self, user_id: str, current_input: str) -> dict:
        """ë™ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""

        context = {
            "current_time": datetime.now().isoformat(),
            "user_timezone": self._get_user_timezone(user_id),
            "recent_events": self._get_recent_events(user_id, days=7),
            "user_patterns": self._get_user_patterns(user_id),
            "preferences": self._get_user_preferences(user_id),
            "conversation_history": self._get_conversation_history(user_id),
            "location_context": self._get_location_context(user_id)
        }

        # ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ìµœì í™”
        optimized_context = self._optimize_context_size(context)

        return optimized_context

    def _optimize_context_size(self, context: dict) -> dict:
        """ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ëª¨ë¸ í•œê³„ì— ë§ê²Œ ìµœì í™”"""

        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        priority_order = [
            "current_time",
            "user_timezone",
            "recent_events",
            "conversation_history",
            "user_patterns",
            "preferences",
            "location_context"
        ]

        optimized = {}
        token_count = 0

        for key in priority_order:
            item_tokens = self._estimate_tokens(context[key])
            if token_count + item_tokens < self.context_window * 0.7:  # 70% í™œìš©
                optimized[key] = context[key]
                token_count += item_tokens
            else:
                # ì¤‘ìš”í•œ ì •ë³´ë§Œ ìš”ì•½í•´ì„œ í¬í•¨
                optimized[key] = self._summarize_context_item(context[key])
                break

        return optimized
```

---

## 5. í•™ìŠµ ë° ê°œì¸í™”

### 5.1 ì‚¬ìš©ì í”¼ë“œë°± í•™ìŠµ

```python
class FeedbackLearningSystem:
    def __init__(self):
        self.feedback_processor = FeedbackProcessor()
        self.pattern_updater = PatternUpdater()
        self.preference_learner = PreferenceLearner()

    async def process_feedback(self, user_id: str, feedback_data: dict):
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ í•™ìŠµ"""

        feedback_analysis_prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ í•™ìŠµ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

ì›ë³¸ ìš”ì²­: {feedback_data['original_request']}
AI ì‘ë‹µ: {feedback_data['ai_response']}
ì‚¬ìš©ì í”¼ë“œë°±: {feedback_data['user_feedback']}
ë§Œì¡±ë„: {feedback_data['satisfaction_score']}/5
ìˆ˜ì • ì‚¬í•­: {feedback_data['corrections']}

ë¶„ì„í•  ì˜ì—­:
1. ìì—°ì–´ ì´í•´ ì •í™•ë„
2. ì‹œê°„ í•´ì„ ì •í™•ë„
3. ì„ í˜¸ë„ íŒŒì•… ì •í™•ë„
4. ì»¨í…ìŠ¤íŠ¸ í™œìš©ë„
5. ì‘ë‹µ í’ˆì§ˆ

í•™ìŠµ ë°©í–¥ì„ JSON í˜•íƒœë¡œ ì œì‹œ:
{{
    "understanding_errors": ["ì˜¤í•´í•œ ë¶€ë¶„ë“¤"],
    "preference_updates": {{"ìƒˆë¡œ í•™ìŠµëœ ì„ í˜¸ë„"}},
    "pattern_insights": ["ë°œê²¬ëœ íŒ¨í„´"],
    "improvement_areas": ["ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­"]
}}
"""

        analysis = await self.solar_llm.agenerate([feedback_analysis_prompt])
        learning_points = json.loads(analysis.generations[0][0].text)

        # í•™ìŠµ ë‚´ìš©ì„ ì‚¬ìš©ì í”„ë¡œí•„ì— ë°˜ì˜
        await self._update_user_profile(user_id, learning_points)

        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
        await self._update_embeddings(user_id, learning_points)
```

### 5.2 ê°œì¸í™” í”„ë¡¬í”„íŠ¸ ìƒì„±

```python
class PersonalizedPromptGenerator:
    def __init__(self):
        self.user_profiles = {}

    def generate_personalized_prompt(self, user_id: str, base_prompt: str) -> str:
        """ì‚¬ìš©ìë³„ ê°œì¸í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        user_profile = self._get_user_profile(user_id)

        personalization_layer = f"""
ì‚¬ìš©ì ê°œì¸í™” ì •ë³´:
- ì„ í˜¸ ì‹œê°„ëŒ€: {user_profile.get('preferred_times', [])}
- ìƒì‚°ì„± íŒ¨í„´: {user_profile.get('productivity_pattern', 'unknown')}
- ì—…ë¬´ ìŠ¤íƒ€ì¼: {user_profile.get('work_style', 'unknown')}
- ìœ„ì¹˜ ì„ í˜¸: {user_profile.get('location_preferences', [])}
- ì•Œë¦¼ ìŠ¤íƒ€ì¼: {user_profile.get('notification_style', 'standard')}
- ê³¼ê±° í”¼ë“œë°±: {user_profile.get('recent_feedback', [])}

ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìµœì í™”ëœ ì‘ë‹µì„ ì œê³µí•˜ì„¸ìš”.
"""

        return f"{base_prompt}\n\n{personalization_layer}"
```

### 5.3 ì‹¤ì‹œê°„ í•™ìŠµ íŒŒì´í”„ë¼ì¸

```python
class RealTimeLearningPipeline:
    def __init__(self):
        self.learning_queue = asyncio.Queue()
        self.batch_processor = BatchProcessor()

    async def continuous_learning(self):
        """ì‹¤ì‹œê°„ ì§€ì† í•™ìŠµ"""

        while True:
            try:
                # ë°°ì¹˜ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
                batch_data = []
                for _ in range(10):  # 10ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
                    if not self.learning_queue.empty():
                        batch_data.append(await self.learning_queue.get())

                if batch_data:
                    # ë°°ì¹˜ ë¶„ì„
                    insights = await self._analyze_batch(batch_data)

                    # ê¸€ë¡œë²Œ íŒ¨í„´ ì—…ë°ì´íŠ¸
                    await self._update_global_patterns(insights)

                    # ê°œë³„ ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
                    await self._update_individual_profiles(batch_data)

                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì‹¤í–‰

            except Exception as e:
                logger.error(f"Learning pipeline error: {e}")
                await asyncio.sleep(60)
```

---

## 6. ì„±ëŠ¥ ìµœì í™”

### 6.1 ì‘ë‹µ ì‹œê°„ ìµœì í™”

```python
class ResponseOptimizer:
    def __init__(self):
        self.cache_manager = CacheManager()
        self.streaming_enabled = True

    async def optimize_response(self, query: str, context: dict) -> str:
        """ì‘ë‹µ ì†ë„ ìµœì í™”"""

        # 1. ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(query, context)
        cached_response = await self.cache_manager.get(cache_key)

        if cached_response and cached_response['confidence'] > 0.9:
            return cached_response['response']

        # 2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        if self.streaming_enabled:
            return await self._stream_response(query, context)

        # 3. ì¼ë°˜ ì‘ë‹µ
        return await self._generate_response(query, context)

    async def _stream_response(self, query: str, context: dict):
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‘ë‹µ ìƒì„±"""

        async def response_generator():
            prompt = self._build_prompt(query, context)

            async for chunk in self.solar_llm.astream(prompt):
                yield chunk.content

        return response_generator()
```

### 6.2 í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”

```python
class TokenOptimizer:
    def __init__(self):
        self.token_limits = {
            'solar-pro': 128000,
            'solar-mini': 32000
        }

    def optimize_prompt(self, prompt: str, model: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í† í° ìµœì í™”"""

        current_tokens = self._count_tokens(prompt)
        limit = self.token_limits.get(model, 32000)

        if current_tokens <= limit * 0.8:  # 80% ì´í•˜ ì‚¬ìš©
            return prompt

        # í”„ë¡¬í”„íŠ¸ ì••ì¶•
        compressed = self._compress_prompt(prompt, limit * 0.7)
        return compressed

    def _compress_prompt(self, prompt: str, target_tokens: int) -> str:
        """í”„ë¡¬í”„íŠ¸ ì§€ëŠ¥í˜• ì••ì¶•"""

        compression_prompt = f"""
ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¥¼ {target_tokens} í† í° ì´í•˜ë¡œ ì••ì¶•í•˜ë˜, í•µì‹¬ ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”:

ì›ë³¸ í”„ë¡¬í”„íŠ¸:
{prompt}

ì••ì¶• ì›ì¹™:
1. í•µì‹¬ ì§€ì‹œì‚¬í•­ ìœ ì§€
2. ì¤‘ë³µ ì •ë³´ ì œê±°
3. ë¶ˆí•„ìš”í•œ ì˜ˆì‹œ ì¶•ì†Œ
4. ìš”ì•½ ë° ì••ì¶•
"""

        # Solar Proë¡œ ì••ì¶• ìˆ˜í–‰
        response = self.solar_llm.generate([compression_prompt])
        return response.generations[0][0].text
```

### 6.3 ëª¨ë¸ ì„ íƒ ì „ëµ

```python
class ModelSelector:
    def __init__(self):
        self.models = {
            'solar-pro': {
                'cost': 0.1,
                'speed': 'medium',
                'capability': 'high',
                'context_window': 128000
            },
            'solar-mini': {
                'cost': 0.05,
                'speed': 'fast',
                'capability': 'medium',
                'context_window': 32000
            },
            'gpt-4': {
                'cost': 0.3,
                'speed': 'slow',
                'capability': 'highest',
                'context_window': 128000
            }
        }

    def select_optimal_model(self, task_type: str, complexity: float, urgency: str) -> str:
        """ì‘ì—… íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ"""

        if task_type == 'simple_scheduling' and complexity < 0.5:
            return 'solar-mini'
        elif task_type == 'complex_optimization' and complexity > 0.8:
            return 'gpt-4'
        else:
            return 'solar-pro'  # ê¸°ë³¸ ëª¨ë¸
```

---

## 7. í’ˆì§ˆ ë³´ì¦ ë° ëª¨ë‹ˆí„°ë§

### 7.1 ì‘ë‹µ í’ˆì§ˆ í‰ê°€

```python
class QualityAssurance:
    def __init__(self):
        self.evaluators = {
            'accuracy': AccuracyEvaluator(),
            'relevance': RelevanceEvaluator(),
            'safety': SafetyEvaluator(),
            'coherence': CoherenceEvaluator()
        }

    async def evaluate_response(self, query: str, response: str, context: dict) -> dict:
        """ì‘ë‹µ í’ˆì§ˆ ì¢…í•© í‰ê°€"""

        evaluation_results = {}

        for metric, evaluator in self.evaluators.items():
            score = await evaluator.evaluate(query, response, context)
            evaluation_results[metric] = score

        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_score(evaluation_results)

        return {
            'individual_scores': evaluation_results,
            'overall_score': overall_score,
            'quality_level': self._get_quality_level(overall_score)
        }

    def _calculate_overall_score(self, scores: dict) -> float:
        """ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        weights = {
            'accuracy': 0.4,
            'relevance': 0.3,
            'safety': 0.2,
            'coherence': 0.1
        }

        return sum(scores[metric] * weight for metric, weight in weights.items())
```

### 7.2 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```python
class LLMMonitoring:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()

    async def monitor_performance(self):
        """LLM ì„±ëŠ¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""

        while True:
            try:
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                metrics = await self._collect_metrics()

                # ì„ê³„ê°’ í™•ì¸
                alerts = self._check_thresholds(metrics)

                if alerts:
                    await self.alert_manager.send_alerts(alerts)

                # ë©”íŠ¸ë¦­ ì €ì¥
                await self._store_metrics(metrics)

                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

            except Exception as e:
                logger.error(f"Monitoring error: {e}")

    async def _collect_metrics(self) -> dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

        return {
            'response_time': await self._get_avg_response_time(),
            'success_rate': await self._get_success_rate(),
            'user_satisfaction': await self._get_satisfaction_score(),
            'token_usage': await self._get_token_usage(),
            'error_rate': await self._get_error_rate(),
            'cache_hit_rate': await self._get_cache_hit_rate()
        }
```

### 7.3 A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ

```python
class ABTestManager:
    def __init__(self):
        self.experiments = {}
        self.traffic_splitter = TrafficSplitter()

    def create_experiment(self, name: str, variants: dict, traffic_split: dict):
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±"""

        self.experiments[name] = {
            'variants': variants,
            'traffic_split': traffic_split,
            'metrics': {},
            'start_time': datetime.now(),
            'status': 'active'
        }

    async def get_variant_for_user(self, experiment_name: str, user_id: str) -> str:
        """ì‚¬ìš©ìë³„ ì‹¤í—˜ ë³€í˜• í• ë‹¹"""

        if experiment_name not in self.experiments:
            return 'control'

        experiment = self.experiments[experiment_name]

        # ì¼ê´€ëœ í• ë‹¹ì„ ìœ„í•œ í•´ì‹œ ê¸°ë°˜ ë¶„í• 
        user_hash = hash(f"{experiment_name}_{user_id}") % 100

        cumulative = 0
        for variant, percentage in experiment['traffic_split'].items():
            cumulative += percentage
            if user_hash < cumulative:
                return variant

        return 'control'
```

---

## 8. ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ë³´í˜¸

### 8.1 ë°ì´í„° ë§ˆìŠ¤í‚¹

```python
class DataMasking:
    def __init__(self):
        self.sensitive_patterns = [
            (r'[\w\.-]+@[\w\.-]+\.\w+', 'EMAIL'),  # ì´ë©”ì¼
            (r'\d{3}-\d{4}-\d{4}', 'PHONE'),       # ì „í™”ë²ˆí˜¸
            (r'\d{6}-\d{7}', 'SSN'),               # ì£¼ë¯¼ë²ˆí˜¸
        ]

    def mask_sensitive_data(self, text: str) -> tuple[str, dict]:
        """ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹"""

        masked_text = text
        mapping = {}

        for pattern, data_type in self.sensitive_patterns:
            matches = re.finditer(pattern, text)
            for i, match in enumerate(matches):
                original = match.group()
                placeholder = f"[{data_type}_{i}]"

                masked_text = masked_text.replace(original, placeholder)
                mapping[placeholder] = original

        return masked_text, mapping

    def unmask_response(self, response: str, mapping: dict) -> str:
        """ì‘ë‹µì—ì„œ ë§ˆìŠ¤í‚¹ í•´ì œ"""

        unmasked_response = response
        for placeholder, original in mapping.items():
            unmasked_response = unmasked_response.replace(placeholder, original)

        return unmasked_response
```

### 8.2 í”„ë¼ì´ë²„ì‹œ ë³´í˜¸

```python
class PrivacyProtector:
    def __init__(self):
        self.anonymizer = DataAnonymizer()
        self.encryptor = DataEncryptor()

    async def protect_user_data(self, user_data: dict) -> dict:
        """ì‚¬ìš©ì ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´í˜¸"""

        # 1. ê°œì¸ì‹ë³„ì •ë³´ ìµëª…í™”
        anonymized_data = self.anonymizer.anonymize(user_data)

        # 2. ë¯¼ê° ì •ë³´ ì•”í˜¸í™”
        encrypted_data = self.encryptor.encrypt_sensitive_fields(anonymized_data)

        # 3. ë°ì´í„° ìµœì†Œí™”
        minimized_data = self._minimize_data(encrypted_data)

        return minimized_data

    def _minimize_data(self, data: dict) -> dict:
        """í•„ìš”í•œ ë°ì´í„°ë§Œ ìœ ì§€"""

        essential_fields = [
            'user_patterns',
            'preferences',
            'schedule_context',
            'anonymized_history'
        ]

        return {k: v for k, v in data.items() if k in essential_fields}
```

---

## 9. ë°°í¬ ë° ìš´ì˜

### 9.1 ëª¨ë¸ ë²„ì „ ê´€ë¦¬

```python
class ModelVersionManager:
    def __init__(self):
        self.versions = {}
        self.deployment_manager = DeploymentManager()

    async def deploy_new_version(self, version: str, model_config: dict):
        """ìƒˆ ëª¨ë¸ ë²„ì „ ë°°í¬"""

        # 1. ì¹´ë‚˜ë¦¬ ë°°í¬ (5% íŠ¸ë˜í”½)
        await self.deployment_manager.deploy_canary(version, model_config, 0.05)

        # 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (24ì‹œê°„)
        monitoring_results = await self._monitor_canary(version, hours=24)

        # 3. ì„±ëŠ¥ ì„ê³„ê°’ í™•ì¸
        if monitoring_results['success_rate'] > 0.95:
            # ì ì§„ì  íŠ¸ë˜í”½ ì¦ê°€
            await self._gradual_rollout(version)
        else:
            # ë¡¤ë°±
            await self.deployment_manager.rollback(version)
```

### 9.2 ì„¤ì • ê´€ë¦¬

```yaml
# llm_config.yaml
models:
  solar_pro:
    endpoint: 'https://api.upstage.ai/v1'
    model_name: 'solar-1-mini-chat'
    max_tokens: 1024
    temperature: 0.1
    timeout: 30
    retry_count: 3

  fallback:
    endpoint: 'https://api.openai.com/v1'
    model_name: 'gpt-4-1106-preview'
    max_tokens: 2048
    temperature: 0.2

performance:
  cache_ttl: 3600 # 1ì‹œê°„
  max_concurrent_requests: 100
  rate_limit: 1000 # requests per minute

monitoring:
  metrics_interval: 60 # seconds
  alert_thresholds:
    response_time: 5000 # ms
    error_rate: 0.05 # 5%
    satisfaction_score: 0.8

privacy:
  data_retention_days: 90
  anonymization_enabled: true
  encryption_enabled: true
```

---

ì´ LLM ëª…ì„¸ì„œëŠ” Solar Pro ëª¨ë¸ê³¼ LangChainì„ í™œìš©í•˜ì—¬ LinQì˜ AI ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê¸°
ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì˜ ì¥ì ì„ ìµœëŒ€í•œ
í™œìš©í•˜ë©´ì„œë„ í™•ì¥ì„±ê³¼ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.
